### üîç **Learn to Search ‚Äì Project Instructions (Updated)**

This project focuses on building a simple, custom search engine using neural networks and a vector database. You'll work through core concepts of information retrieval using embedding models and similarity search.

---

## üîß Key Steps

### 1. **Tokenizer**
Build your own tokenizer:
- Split text into words or subwords.
- Convert each token into an **index** instead of using the raw token.

---

### 2. **CBOW Embedding**
Implement a simple **CBOW (Continuous Bag of Words)** model:
- Take the average of token embeddings for the entire input.
- Use this to represent queries and documents as vectors.

---

### 3. **Triplet Dataset**
Create a dataset with the structure:
- **(query, positive document, negative document)**
- Each triplet should reflect a true match (positive) and a mismatch (negative).

---

### 4. **Training Model ‚Äì Dual Tower Architecture (OFFLINE)**  
Train two separate neural networks ("towers"):
- **Query tower** and **Document tower**
- Use the CBOW embedding inside both towers.
- Apply **triplet loss** to ensure that for a given query:
  - The positive document is **closer** than the negative document in vector space.

---

### 5. **Document Encoding and Indexing (OFFLINE)**  
Once trained:
- Use the **document tower** to convert all documents into fixed-size vectors.
- Store these vectors in a **vector database**, like **ChromaDB**.
- Each vector corresponds to one document.

---

### 6. **Query Encoding and Search (ONLINE)**  
When a user inputs a query:
- Pass it through the **query tower** to get a query vector.
- Use **Approximate Nearest Neighbor (ANN)** search to find the most similar document vectors stored in ChromaDB.
- Return the top match(es).

---

### 7. **Simple Search App**
Use **Streamlit** (or similar) to build a small app:
- Text input field for queries.
- Output the most relevant document(s) using your trained search engine.