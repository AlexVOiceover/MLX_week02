W&B initialized: ltr-model-20250421-170951
Loading pre-trained embedding model...
Loaded embedding model with 111712 words, dimension 100
Loaded embedding model with vocabulary size: 111712
Creating model towers...
Model towers created successfully.
Setting up dataset and dataloader...
Dataset created with 575739 triplets
Dataloader created with batch size 32
Setting up optimizer...
Optimizer created with learning rate 0.001
Total trainable parameters: 20200
Starting training...
Epoch 1/10
  Average loss: 0.1728
Epoch 2/10
Traceback (most recent call last):
  File "/Users/alex/Documents/MLX/MLX_week02/src/training/train.py", line 183, in <module>
    train()
  File "/Users/alex/Documents/MLX/MLX_week02/src/training/train.py", line 118, in train
    for query_batch, pos_doc_batch, neg_doc_batch in dataloader:
                                                     ^^^^^^^^^^
  File "/Users/alex/Documents/MLX/MLX_week02/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/Users/alex/Documents/MLX/MLX_week02/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/alex/Documents/MLX/MLX_week02/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/Users/alex/Documents/MLX/MLX_week02/src/data/triplet_dataset.py", line 83, in __getitem__
    negative_tensor = self._process_text(negative_text, self.max_length)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/alex/Documents/MLX/MLX_week02/src/data/triplet_dataset.py", line 61, in _process_text
    tokens = tokenize_text(text)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/alex/Documents/MLX/MLX_week02/src/utils/tokenization.py", line 21, in tokenize_text
    text = re.sub(r'[^a-z0-9]', ' ', text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/re/__init__.py", line 186, in sub
    return _compile(pattern, flags).sub(repl, string, count)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
