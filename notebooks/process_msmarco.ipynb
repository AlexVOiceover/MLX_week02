{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS MARCO Data Download and Processing\n",
    "\n",
    "This notebook handles both downloading and processing MS MARCO data for our Learning to Rank project:\n",
    "\n",
    "1. Downloads raw data from HuggingFace (if not already downloaded)\n",
    "2. Processes the data into JSON format for training\n",
    "3. Deduplicates passages to ensure each unique passage text is assigned only one ID\n",
    "\n",
    "**Note**: If you already have the required files in the data/raw directory, the download step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset files\n",
    "DATASETS = [\"train\", \"validation\", \"test\"]\n",
    "DATASET_FILES = {dataset: RAW_DIR / f\"{dataset}-00000-of-00001.parquet\" for dataset in DATASETS}\n",
    "\n",
    "# Sample size for development (set to None for full dataset)\n",
    "SAMPLE_SIZE = None  # Example: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for MS MARCO dataset files...\n",
      "All dataset files already exist! Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download function\n",
    "def download_ms_marco_data():\n",
    "    \"\"\"Download MS MARCO dataset files from HuggingFace if they don't already exist.\"\"\"\n",
    "    print(\"Checking for MS MARCO dataset files...\")\n",
    "    \n",
    "    # Check which files need to be downloaded\n",
    "    missing_files = [f for dataset, f in DATASET_FILES.items() if not f.exists()]\n",
    "    \n",
    "    if not missing_files:\n",
    "        print(\"All dataset files already exist! Skipping download.\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"Need to download {len(missing_files)} files.\")\n",
    "    \n",
    "    # Download missing files\n",
    "    for dataset, file_path in DATASET_FILES.items():\n",
    "        if file_path.exists():\n",
    "            print(f\"  {dataset} dataset already exists, skipping download\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"  Downloading {dataset} dataset...\")\n",
    "            \n",
    "            # Direct URL to the dataset file\n",
    "            url = f\"https://huggingface.co/datasets/microsoft/ms_marco/resolve/main/v1.1/{dataset}-00000-of-00001.parquet\"\n",
    "            \n",
    "            # Download with progress bar\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Get total file size for progress bar\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Use tqdm for a progress bar\n",
    "            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"  {dataset}\")\n",
    "            \n",
    "            # Save the file\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:  # filter out keep-alive chunks\n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "            \n",
    "            progress_bar.close()\n",
    "            print(f\"  {dataset} dataset downloaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {dataset} dataset: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Clean up any cache folders that might have been created\n",
    "    cleanup_paths = [\n",
    "        Path(\".cache\"),\n",
    "        Path(tempfile.gettempdir()) / \"huggingface\"\n",
    "    ]\n",
    "    \n",
    "    for path in cleanup_paths:\n",
    "        if path.exists() and path.is_dir():\n",
    "            try:\n",
    "                print(f\"Cleaning up cache directory: {path}\")\n",
    "                shutil.rmtree(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not clean up {path}: {e}\")\n",
    "    \n",
    "    print(\"Download complete!\")\n",
    "    return True\n",
    "\n",
    "# Run download\n",
    "download_successful = download_ms_marco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/train-00000-of-00001.parquet...\n",
      "Loaded 82326 rows\n"
     ]
    }
   ],
   "source": [
    "# Verify the download was successful\n",
    "if not download_successful:\n",
    "    raise Exception(\"Download failed. Please check the error messages above.\")\n",
    "\n",
    "# Set train file for processing\n",
    "train_file = DATASET_FILES[\"train\"]\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data from {train_file}...\")\n",
    "df = pd.read_parquet(train_file)\n",
    "if SAMPLE_SIZE is not None:\n",
    "    df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Loaded {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 82326/82326 [00:07<00:00, 11401.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 82326 queries, 608265 unique passages, and 79704 matches\n",
      "Found 47178 duplicate passages out of 676193 total passages\n",
      "Deduplication rate: 6.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process data with deduplication\n",
    "queries = {}\n",
    "passages = {}\n",
    "matches = {}\n",
    "passage_id_counter = 0\n",
    "\n",
    "# Dictionary to track text to ID mapping for deduplication\n",
    "text_to_id = {}\n",
    "\n",
    "# Stats for reporting\n",
    "duplicate_count = 0\n",
    "total_passages_processed = 0\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing data\"):\n",
    "    query_id = str(row['query_id'])\n",
    "    query_text = row['query']\n",
    "    \n",
    "    # Add query to queries dictionary\n",
    "    queries[query_id] = query_text\n",
    "    \n",
    "    # Process passages\n",
    "    passage_texts = row['passages']['passage_text']\n",
    "    is_selected = row['passages']['is_selected']\n",
    "    total_passages_processed += len(passage_texts)\n",
    "    \n",
    "    # Find selected passage index\n",
    "    selected_idx = None\n",
    "    for i, val in enumerate(is_selected):\n",
    "        if val == 1:\n",
    "            selected_idx = i\n",
    "            break\n",
    "    \n",
    "    if selected_idx is not None:\n",
    "        # We'll collect passage IDs for this query\n",
    "        query_passage_ids = []\n",
    "        \n",
    "        # Process each passage for this query\n",
    "        for i, text in enumerate(passage_texts):\n",
    "            # Check if we've seen this passage before\n",
    "            if text in text_to_id:\n",
    "                # Reuse existing ID\n",
    "                pid = text_to_id[text]\n",
    "                duplicate_count += 1\n",
    "            else:\n",
    "                # Create new ID for this unique passage\n",
    "                pid = f\"p{passage_id_counter}\"\n",
    "                passage_id_counter += 1\n",
    "                \n",
    "                # Store in our mapping and passages dictionary\n",
    "                text_to_id[text] = pid\n",
    "                passages[pid] = text\n",
    "            \n",
    "            # Add this passage ID to the query's passage list\n",
    "            query_passage_ids.append(pid)\n",
    "        \n",
    "        # Create match entry - using deduplicated IDs\n",
    "        matches[query_id] = {\n",
    "            \"suggested\": query_passage_ids,\n",
    "            \"selected\": query_passage_ids[selected_idx]\n",
    "        }\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Processed {len(queries)} queries, {len(passages)} unique passages, and {len(matches)} matches\")\n",
    "print(f\"Found {duplicate_count} duplicate passages out of {total_passages_processed} total passages\")\n",
    "print(f\"Deduplication rate: {duplicate_count / total_passages_processed * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to JSON files...\n",
      "Data saved to:\n",
      "- ../data/processed/queries.json\n",
      "- ../data/processed/passages.json\n",
      "- ../data/processed/matches.json\n"
     ]
    }
   ],
   "source": [
    "# Remove existing files if they exist to ensure clean data\n",
    "for filename in [\"queries.json\", \"passages.json\", \"matches.json\"]:\n",
    "    filepath = PROCESSED_DIR / filename\n",
    "    if filepath.exists():\n",
    "        print(f\"Removing existing file: {filepath}\")\n",
    "        os.remove(filepath)\n",
    "\n",
    "# Save data to JSON files\n",
    "print(\"Saving data to JSON files...\")\n",
    "\n",
    "with open(PROCESSED_DIR / \"queries.json\", 'w') as f:\n",
    "    json.dump(queries, f)\n",
    "    \n",
    "with open(PROCESSED_DIR / \"passages.json\", 'w') as f:\n",
    "    json.dump(passages, f)\n",
    "    \n",
    "with open(PROCESSED_DIR / \"matches.json\", 'w') as f:\n",
    "    json.dump(matches, f)\n",
    "    \n",
    "print(f\"Data saved to:\\n- {PROCESSED_DIR / 'queries.json'}\\n- {PROCESSED_DIR / 'passages.json'}\\n- {PROCESSED_DIR / 'matches.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
