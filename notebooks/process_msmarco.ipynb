{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS MARCO Data Download and Processing\n",
    "\n",
    "This notebook handles both downloading and processing MS MARCO data for our Learning to Rank project:\n",
    "\n",
    "1. Downloads raw data from HuggingFace (if not already downloaded)\n",
    "2. Processes the data into JSON format for training\n",
    "\n",
    "**Note**: If you already have the required files in the data/raw directory, the download step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # For notebook progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset files\n",
    "DATASETS = [\"train\", \"validation\", \"test\"]\n",
    "DATASET_FILES = {dataset: RAW_DIR / f\"{dataset}-00000-of-00001.parquet\" for dataset in DATASETS}\n",
    "\n",
    "# Sample size for development (set to None for full dataset)\n",
    "SAMPLE_SIZE = None  # Example: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for MS MARCO dataset files...\n",
      "All dataset files already exist! Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download function\n",
    "def download_ms_marco_data():\n",
    "    \"\"\"Download MS MARCO dataset files from HuggingFace if they don't already exist.\"\"\"\n",
    "    print(\"Checking for MS MARCO dataset files...\")\n",
    "    \n",
    "    # Check which files need to be downloaded\n",
    "    missing_files = [f for dataset, f in DATASET_FILES.items() if not f.exists()]\n",
    "    \n",
    "    if not missing_files:\n",
    "        print(\"All dataset files already exist! Skipping download.\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"Need to download {len(missing_files)} files.\")\n",
    "    \n",
    "    # Download missing files\n",
    "    for dataset, file_path in DATASET_FILES.items():\n",
    "        if file_path.exists():\n",
    "            print(f\"  {dataset} dataset already exists, skipping download\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"  Downloading {dataset} dataset...\")\n",
    "            \n",
    "            # Direct URL to the dataset file\n",
    "            url = f\"https://huggingface.co/datasets/microsoft/ms_marco/resolve/main/v1.1/{dataset}-00000-of-00001.parquet\"\n",
    "            \n",
    "            # Download with progress bar\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Get total file size for progress bar\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Use tqdm for a progress bar\n",
    "            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"  {dataset}\")\n",
    "            \n",
    "            # Save the file\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:  # filter out keep-alive chunks\n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "            \n",
    "            progress_bar.close()\n",
    "            print(f\"  {dataset} dataset downloaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {dataset} dataset: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Clean up any cache folders that might have been created\n",
    "    cleanup_paths = [\n",
    "        Path(\".cache\"),\n",
    "        Path(tempfile.gettempdir()) / \"huggingface\"\n",
    "    ]\n",
    "    \n",
    "    for path in cleanup_paths:\n",
    "        if path.exists() and path.is_dir():\n",
    "            try:\n",
    "                print(f\"Cleaning up cache directory: {path}\")\n",
    "                shutil.rmtree(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not clean up {path}: {e}\")\n",
    "    \n",
    "    print(\"Download complete!\")\n",
    "    return True\n",
    "\n",
    "# Run download\n",
    "download_successful = download_ms_marco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the download was successful\n",
    "if not download_successful:\n",
    "    raise Exception(\"Download failed. Please check the error messages above.\")\n",
    "\n",
    "# Set train file for processing\n",
    "train_file = DATASET_FILES[\"train\"]\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data from {train_file}...\")\n",
    "df = pd.read_parquet(train_file)\n",
    "if SAMPLE_SIZE is not None:\n",
    "    df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Loaded {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761142cd63a24720981ac5ad0615f3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data:   0%|          | 0/82326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 82326 queries, 655443 passages, and 79704 matches\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "queries = {}\n",
    "passages = {}\n",
    "matches = {}\n",
    "passage_id_counter = 0\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing data\"):\n",
    "    query_id = str(row['query_id'])\n",
    "    query_text = row['query']\n",
    "    \n",
    "    # Add query to queries dictionary\n",
    "    queries[query_id] = query_text\n",
    "    \n",
    "    # Process passages\n",
    "    passage_texts = row['passages']['passage_text']\n",
    "    is_selected = row['passages']['is_selected']\n",
    "    \n",
    "    # Find selected passage index\n",
    "    selected_idx = None\n",
    "    for i, val in enumerate(is_selected):\n",
    "        if val == 1:\n",
    "            selected_idx = i\n",
    "            break\n",
    "    \n",
    "    if selected_idx is not None:\n",
    "        # Create unique IDs for passages\n",
    "        passage_ids = [f\"p{passage_id_counter + i}\" for i in range(len(passage_texts))]\n",
    "        \n",
    "        # Add passages to dictionary\n",
    "        for pid, text in zip(passage_ids, passage_texts):\n",
    "            passages[pid] = text\n",
    "        \n",
    "        # Create match entry\n",
    "        matches[query_id] = {\n",
    "            \"suggested\": passage_ids,\n",
    "            \"selected\": passage_ids[selected_idx]\n",
    "        }\n",
    "        \n",
    "        # Update counter\n",
    "        passage_id_counter += len(passage_texts)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Processed {len(queries)} queries, {len(passages)} passages, and {len(matches)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to JSON files...\n",
      "Data saved to:\n",
      "- ../data/processed/queries.json\n",
      "- ../data/processed/passages.json\n",
      "- ../data/processed/matches.json\n"
     ]
    }
   ],
   "source": [
    "# Save data to JSON files\n",
    "print(\"Saving data to JSON files...\")\n",
    "\n",
    "with open(PROCESSED_DIR / \"queries.json\", 'w') as f:\n",
    "    json.dump(queries, f)\n",
    "    \n",
    "with open(PROCESSED_DIR / \"passages.json\", 'w') as f:\n",
    "    json.dump(passages, f)\n",
    "    \n",
    "with open(PROCESSED_DIR / \"matches.json\", 'w') as f:\n",
    "    json.dump(matches, f)\n",
    "    \n",
    "print(f\"Data saved to:\\n- {PROCESSED_DIR / 'queries.json'}\\n- {PROCESSED_DIR / 'passages.json'}\\n- {PROCESSED_DIR / 'matches.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample triplet\n",
    "def generate_sample_triplet():\n",
    "    if not matches:\n",
    "        return None\n",
    "        \n",
    "    # Get a random query\n",
    "    query_id = list(matches.keys())[0]\n",
    "    match = matches[query_id]\n",
    "    \n",
    "    # Get positive passage\n",
    "    positive_id = match[\"selected\"]\n",
    "    \n",
    "    # Get a negative passage\n",
    "    negative_ids = [pid for pid in match[\"suggested\"] if pid != positive_id]\n",
    "    if not negative_ids:\n",
    "        return None\n",
    "    negative_id = negative_ids[0]\n",
    "    \n",
    "    # Create triplet\n",
    "    return {\n",
    "        \"query\": queries[query_id],\n",
    "        \"positive\": passages[positive_id],\n",
    "        \"negative\": passages[negative_id]\n",
    "    }\n",
    "\n",
    "triplet = generate_sample_triplet()\n",
    "if triplet:\n",
    "    print(\"Sample triplet:\")\n",
    "    print(f\"Query: {triplet['query']}\")\n",
    "    pos_text = triplet['positive'][:100] + \"...\" if len(triplet['positive']) > 100 else triplet['positive']\n",
    "    neg_text = triplet['negative'][:100] + \"...\" if len(triplet['negative']) > 100 else triplet['negative']\n",
    "    print(f\"Positive: {pos_text}\")\n",
    "    print(f\"Negative: {neg_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
