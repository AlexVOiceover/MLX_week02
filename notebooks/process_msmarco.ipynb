{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MS MARCO Data Processing\n",
    "\n",
    "This notebook processes MS MARCO data from parquet files into a JSON format for our Learning to Rank project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input files\n",
    "train_file = RAW_DIR / \"train-00000-of-00001.parquet\"\n",
    "\n",
    "# Sample size for development (set to None for full dataset)\n",
    "SAMPLE_SIZE = None  # Example: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/train-00000-of-00001.parquet...\n",
      "Loaded 82326 rows\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(f\"Loading data from {train_file}...\")\n",
    "df = pd.read_parquet(train_file)\n",
    "if SAMPLE_SIZE is not None:\n",
    "    df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Loaded {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 19699\n",
      "Query text: what is rba\n",
      "Passages structure:\n",
      "dict_keys(['is_selected', 'passage_text', 'url'])\n",
      "Number of passages: 10\n"
     ]
    }
   ],
   "source": [
    "# Look at a sample to understand the structure\n",
    "sample = df.iloc[0]\n",
    "print(f\"Query ID: {sample['query_id']}\")\n",
    "print(f\"Query text: {sample['query']}\")\n",
    "print(\"Passages structure:\")\n",
    "print(sample['passages'].keys())\n",
    "print(f\"Number of passages: {len(sample['passages']['passage_text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761142cd63a24720981ac5ad0615f3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data:   0%|          | 0/82326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 82326 queries, 655443 passages, and 79704 matches\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "queries = {}\n",
    "passages = {}\n",
    "matches = {}\n",
    "passage_id_counter = 0\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing data\"):\n",
    "    query_id = str(row['query_id'])\n",
    "    query_text = row['query']\n",
    "    \n",
    "    # Add query to queries dictionary\n",
    "    queries[query_id] = query_text\n",
    "    \n",
    "    # Process passages\n",
    "    passage_texts = row['passages']['passage_text']\n",
    "    is_selected = row['passages']['is_selected']\n",
    "    \n",
    "    # Find selected passage index\n",
    "    selected_idx = None\n",
    "    for i, val in enumerate(is_selected):\n",
    "        if val == 1:\n",
    "            selected_idx = i\n",
    "            break\n",
    "    \n",
    "    if selected_idx is not None:\n",
    "        # Create unique IDs for passages\n",
    "        passage_ids = [f\"p{passage_id_counter + i}\" for i in range(len(passage_texts))]\n",
    "        \n",
    "        # Add passages to dictionary\n",
    "        for pid, text in zip(passage_ids, passage_texts):\n",
    "            passages[pid] = text\n",
    "        \n",
    "        # Create match entry\n",
    "        matches[query_id] = {\n",
    "            \"suggested\": passage_ids,\n",
    "            \"selected\": passage_ids[selected_idx]\n",
    "        }\n",
    "        \n",
    "        # Update counter\n",
    "        passage_id_counter += len(passage_texts)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Processed {len(queries)} queries, {len(passages)} passages, and {len(matches)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to JSON files...\n",
      "Data saved to:\n",
      "- ../data/processed/queries.json\n",
      "- ../data/processed/passages.json\n",
      "- ../data/processed/matches.json\n"
     ]
    }
   ],
   "source": [
    "# Save data to JSON files\n",
    "print(\"Saving data to JSON files...\")\n",
    "\n",
    "with open(PROCESSED_DIR / \"queries.json\", 'w') as f:\n",
    "    json.dump(queries, f)\n",
    "    \n",
    "with open(PROCESSED_DIR / \"passages.json\", 'w') as f:\n",
    "    json.dump(passages, f)\n",
    "    \n",
    "with open(PROCESSED_DIR / \"matches.json\", 'w') as f:\n",
    "    json.dump(matches, f)\n",
    "    \n",
    "print(f\"Data saved to:\\n- {PROCESSED_DIR / 'queries.json'}\\n- {PROCESSED_DIR / 'passages.json'}\\n- {PROCESSED_DIR / 'matches.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample triplet\n",
    "def generate_sample_triplet():\n",
    "    if not matches:\n",
    "        return None\n",
    "        \n",
    "    # Get a random query\n",
    "    query_id = list(matches.keys())[0]\n",
    "    match = matches[query_id]\n",
    "    \n",
    "    # Get positive passage\n",
    "    positive_id = match[\"selected\"]\n",
    "    \n",
    "    # Get a negative passage\n",
    "    negative_ids = [pid for pid in match[\"suggested\"] if pid != positive_id]\n",
    "    if not negative_ids:\n",
    "        return None\n",
    "    negative_id = negative_ids[0]\n",
    "    \n",
    "    # Create triplet\n",
    "    return {\n",
    "        \"query\": queries[query_id],\n",
    "        \"positive\": passages[positive_id],\n",
    "        \"negative\": passages[negative_id]\n",
    "    }\n",
    "\n",
    "triplet = generate_sample_triplet()\n",
    "if triplet:\n",
    "    print(\"Sample triplet:\")\n",
    "    print(f\"Query: {triplet['query']}\")\n",
    "    pos_text = triplet['positive'][:100] + \"...\" if len(triplet['positive']) > 100 else triplet['positive']\n",
    "    neg_text = triplet['negative'][:100] + \"...\" if len(triplet['negative']) > 100 else triplet['negative']\n",
    "    print(f\"Positive: {pos_text}\")\n",
    "    print(f\"Negative: {neg_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
